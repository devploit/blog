<feed xmlns="http://www.w3.org/2005/Atom"> <id>https://blog.0xdev.eu//</id><title>devploit</title><subtitle>Infosec Enthusiast &amp; CTF player.</subtitle> <updated>2024-07-04T07:13:57+00:00</updated> <author> <name>Daniel Púa</name> <uri>https://blog.0xdev.eu//</uri> </author><link rel="self" type="application/atom+xml" href="https://blog.0xdev.eu//feed.xml"/><link rel="alternate" type="text/html" hreflang="en" href="https://blog.0xdev.eu//"/> <generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator> <rights> © 2024 Daniel Púa </rights> <icon>/assets/img/favicons/favicon.ico</icon> <logo>/assets/img/favicons/favicon-96x96.png</logo> <entry><title>Cracking Gandalf: Conquering the Lakera AI Security Challenge</title><link href="https://blog.0xdev.eu//posts/cracking-gandalf-conquering-the-lakera-ai-security-challenge/" rel="alternate" type="text/html" title="Cracking Gandalf: Conquering the Lakera AI Security Challenge" /><published>2024-07-03T00:00:00+00:00</published> <updated>2024-07-04T07:13:29+00:00</updated> <id>https://blog.0xdev.eu//posts/cracking-gandalf-conquering-the-lakera-ai-security-challenge/</id> <content src="https://blog.0xdev.eu//posts/cracking-gandalf-conquering-the-lakera-ai-security-challenge/" /> <author> <name>Daniel Púa</name> </author> <category term="AI Security" /> <summary> Gandalf by Lakera is an engaging and educational online challenge designed to test and improve your skills in manipulating large language models (LLMs). Named after the wise wizard from “The Lord of the Rings”, this game involves progressively difficult levels where you must use clever prompts to make the AI reveal a secret password. The challenge is not just a fun exercise; it also serves to ... </summary> </entry> <entry><title>Hacking the Mind of AI: Pentesting Large Language Models</title><link href="https://blog.0xdev.eu//posts/hacking-the-mind-of-ai-pentesting-large-language-models/" rel="alternate" type="text/html" title="Hacking the Mind of AI: Pentesting Large Language Models" /><published>2024-06-25T00:00:00+00:00</published> <updated>2024-06-25T20:27:01+00:00</updated> <id>https://blog.0xdev.eu//posts/hacking-the-mind-of-ai-pentesting-large-language-models/</id> <content src="https://blog.0xdev.eu//posts/hacking-the-mind-of-ai-pentesting-large-language-models/" /> <author> <name>Daniel Púa</name> </author> <category term="AI Security" /> <summary> Pentesting Large Language Models (LLMs) is crucial to ensure they operate securely and do not expose vulnerabilities that can be exploited by attackers. Based on OWASP’s Top 10 vulnerabilities for LLM applications, this post details each vulnerability, examples of exploitation, and mitigation measures. What is an LLM? Large Language Models (LLMs) are AI algorithms designed to understand and... </summary> </entry> </feed>
